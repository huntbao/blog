---
title: 大模型的训练成本
category: llm
tag: llm
permalink: llm-training-costs
---

11 月 6 号， Kimi 推出了 K2 Thinking 模型，它是一款开源的推理模型，官方称之为是 Agent Model，能在无人工干预的情况下，实现 200 到 300 次的一系列工具调用，上下文窗口高达 256k。坊间传闻 K2 Thinking 的训练成本为 460 万美元。

这个数字让我想起了 OpenAI 在 2023 年 1 月份公布的 GPT-4 的训练成本，OpenAI 当时表示 GPT-4 的训练成本大约为 1 亿美元，相比之下，K2 Thinking 的训练成本显得相当低廉。

通过查阅资料，发现大模型的训练成本主要包括以下几个方面：

1. 硬件成本（计算成本）
2. 数据成本
3. 研发与人力成本
4. 电力和基础设施成本
5. 实验与试错成本

## 硬件成本

这是最直观、通常也是最大的一块费用。训练大模型需要进行数万亿甚至数千万亿次的计算，这需要强大的计算硬件。

核心硬件是 GPU，GPU 拥有数千个计算核心，特别适合进行大模型训练所需的并行矩阵运算，其效率远高于 CPU。目前主流使用的是 NVIDIA 的 A100/H100，以及之前的 V100。这些专业级 GPU 单价非常昂贵，一台 H100 服务器的价格可能高达 20-30 万美元。训练一个千亿参数模型可能需要数千甚至上万个 GPU 同时工作数周或数月。

大多数公司选择从云服务商（如 AWS、Google Cloud、Azure）租赁 GPU 算力。费用按使用时长计算。训练一个大模型的云服务账单可以达到数百万甚至上千万美元。

能用 CPU 训练大模型吗？不是绝对不能，而是极端不切实际，效率低到无法接受。CPU 像一辆超级跑车。它非常擅长快速、连续地处理复杂任务（比如在一条空荡荡的高速公路上飞驰）。它的核心数量少（比如 8 核、16 核），但每个核心都非常强大，能独立处理复杂指令。GPU 像一个庞大的货运车队。它由成千上万个“小核心”组成（例如，NVIDIA H100 有上万核心）。每个核心都很简单、很慢，但它们可以同时做大量简单的计算。大模型训练的计算本质是大规模的并行矩阵运算，这正是 GPU 的强项。说个白话的比喻：用 CPU 训练大模型，就像让一位诺贝尔奖得主去工地一块一块地手工砌砖来盖摩天大楼。虽然他能理解整个建筑学的原理（通用逻辑能力强），但这个过程成本高昂、效率极低，是对其才华的巨大浪费。而使用 GPU，则像是调动了一支成千上万的机械化工程兵团，每个士兵（计算核心）只负责执行简单的标准化操作，但通过完美的协同和极高的吞吐量，能以惊人的速度完成建设。这才是建造 AI 高楼的正确方式。

## 数据成本

高质量的训练数据是模型性能的基石，而获取和处理这些数据的成本极高。

数据获取：

- 购买数据集： 向数据提供商购买高质量的专有数据集（如特定领域的学术论文、书籍、代码库等）。
- 网络爬取： 从互联网上爬取海量文本、图像数据。这涉及到带宽成本、存储成本以及法律合规风险。
- 数据标注： 对于监督学习或指令微调阶段，需要人工对数据进行清洗、分类和标注。雇佣标注团队是一笔巨大开销。例如，为训练一个遵循指令的模型，可能需要雇佣成千上万的标注员。

数据处理：

- 清洗与去重： 原始数据包含大量噪声、重复和低质内容，需要强大的计算资源进行预处理。
- tokenization（分词）：将文本转换成模型能理解的数字序列，这个过程本身也需要消耗计算资源。

## 研发与人力成本

训练大模型需要一个顶尖的团队。

团队构成与薪资：

- AI 研究员/科学家： 负责设计模型架构、创新训练算法、解决训练中的难题。他们是全球最稀缺的人才，年薪可达数十万甚至上百万美元。
- 机器学习工程师： 负责将研究想法落地，搭建高效、稳定的训练和推理系统。
- 数据工程师： 负责构建和维护数据管道。
- 运维工程师（SRE）： 负责维护庞大的 GPU 集群，确保其高可用性。
- 一个完整的团队可能多达数十甚至数百人，其总人力成本极其高昂。

研发时间：从构想到最终训练出一个可用的模型，可能需要长达数年的研究和实验周期，这段时间的所有投入都是成本。

## 电力和基础设施成本

运行成千上万个 GPU 会产生巨大的能源消耗和配套设施需求。

电力消耗：

- 一个 GPU 的功耗通常在 300W 到 700W 之间。数千个 GPU 同时运行，其总功耗相当于一个小型城镇的用电量。
- 训练一个模型所消耗的电力成本可能高达数十万甚至上百万美元。

基础设施：

- 数据中心： 需要专业的数据中心来容纳这些服务器，包括机柜、制冷系统（强大的 GPU 会产生大量热量，需要液冷等高效散热方案）、不同断电源（UPS）和备用发电机。
- 网络： GPU 之间需要高速网络进行通信，以减少训练时的等待时间。这些网络设备非常昂贵。

## 实验与试错成本

大模型的训练不是一蹴而就的，而是一个不断实验和迭代的过程。

- 超参数调优：学习率、批大小等超参数对训练结果影响巨大。研究人员需要多次运行“小规模”实验来寻找最优配置，每一次实验都消耗着 GPU 时间和电力。
- 架构搜索：尝试不同的模型层数、注意力头数等架构变化，每次尝试都相当于训练一个完整的模型。
- 训练失败：由于软件 bug、硬件故障或训练过程本身发散（如梯度爆炸），一次长达数周的训练可能中途失败，所有已投入的计算资源全部浪费。这种风险成本也必须计算在内。

综上所述，大模型的训练成本是一个复杂且多维度的计算，涉及硬件、数据、人力、电力和实验等多个方面，每一项都可能高达数百万美元。不管 K2 Thinking 的具体训练成本是多少，但相信肯 Kimi 应该在各个方面都做了大量优化和权衡，才能将成本控制在相对较低的水平。

## 题外话

目前世面上，GPU 以 NVIDIA 一家独大，AMD 和 Intel 虽然也推出了自己的 GPU，但市场份额和生态支持远不及 NVIDIA。

NVIDIA 的强大不仅仅在于硬件，更在于其构建的、几乎无法复制的软件和生态系统壁垒：

- 硬件：其 CUDA 核心和专用的 Tensor Core 在 AI 计算上经过多代迭代，非常成熟高效。
- 软件生态：这是其最核心的护城河
  - CUDA：这不仅仅是一个编程语言，而是一个完整的平台。几乎所有 AI 框架（PyTorch, TensorFlow）都深度集成 CUDA，绝大多数 AI 研究员和工程师都只会用 CUDA 编程。这种用户习惯和社区惯性是巨大的优势。
  - cuDNN, TensorRT 等库：为深度学习提供了极度优化的底层计算库。

全栈解决方案：从数据中心（DGX 超算）到网络（InfiniBand），再到软件和云服务，NVIDIA 提供了一站式解决方案。

## 结语

希望国内的开源大模型能在 AI 时代激起更多的浪花。